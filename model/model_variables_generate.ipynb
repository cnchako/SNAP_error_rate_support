{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7a0a4f2a-71aa-4a58-bc6e-3a962b47daee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254637, 855)\n",
      "year\n",
      "2017    45530\n",
      "2018    43738\n",
      "2019    43258\n",
      "2020    27112\n",
      "2021     9832\n",
      "2022    41391\n",
      "2023    43776\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\27311\\AppData\\Local\\Temp\\ipykernel_26632\\2190034180.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[\"error_flag\"] = (\n"
     ]
    }
   ],
   "source": [
    "# Preparation part\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# load the data\n",
    "\n",
    "df17 = pd.read_stata(\"qc_pub_fy2017.dta\").assign(year=2017)\n",
    "df18 = pd.read_stata(\"qc_pub_fy2018.dta\").assign(year=2018)\n",
    "df19 = pd.read_stata(\"qc_pub_fy2019.dta\").assign(year=2019)\n",
    "df20 = pd.read_stata(\"qc_pub_fy2020.dta\").assign(year=2020)\n",
    "df21 = pd.read_stata(\"qc_pub_fy2021.dta\").assign(year=2021)\n",
    "df22 = pd.read_stata(\"qc_pub_fy2022.dta\").assign(year=2022)\n",
    "df23 = pd.read_stata(\"qc_pub_fy2023.dta\").assign(year=2023)\n",
    "\n",
    "\n",
    "# defing a clean function \n",
    "\n",
    "def clean_columns(df):\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()                      \n",
    "        .str.lower()                     \n",
    "        .str.replace(r'[\\s\\.-]+', '_', regex=True)  \n",
    "        .str.replace(r'__+', '_', regex=True)        \n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# clean all the df for each year\n",
    "\n",
    "df17 = clean_columns(df17)\n",
    "df18 = clean_columns(df18)\n",
    "df19 = clean_columns(df19)\n",
    "df20 = clean_columns(df20)\n",
    "df21 = clean_columns(df21)\n",
    "df22 = clean_columns(df22)\n",
    "df23 = clean_columns(df23)\n",
    "\n",
    "\n",
    "\n",
    "# concat all the data\n",
    "df_all = pd.concat([\n",
    "    df17, df18, df19, df20, df21, df22, df23\n",
    "], ignore_index=True, join=\"outer\")\n",
    "\n",
    "# check the shape\n",
    "print(df_all.shape)\n",
    "print(df_all[\"year\"].value_counts().sort_index())\n",
    "\n",
    "# create error_flag variable\n",
    "find_cols = [f\"element{i}\" for i in range(1, 10)]\n",
    "\n",
    "df_all[\"error_flag\"] = (\n",
    "    df_all[find_cols]\n",
    "    .apply(lambda row: 1 if row.notna().any() else 0, axis = 1)\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "61a7fdb0-208f-4259-a41e-e043049acefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\"raw_data_2017-2023.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c487443-dac8-453e-9bdc-4138df13784d",
   "metadata": {},
   "source": [
    "=================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5b77d240-26bb-4ac5-ac0f-d5f881d66590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Total_Cases  Error_Cases  Error_Rate\n",
      "year                                      \n",
      "2017        45530        16014      0.3517\n",
      "2018        43738        15506      0.3545\n",
      "2019        43258        15388      0.3557\n",
      "2020        27112         9322      0.3438\n",
      "2021         9832         3463      0.3522\n",
      "2022        41391        15611      0.3772\n",
      "2023        43776        16906      0.3862\n"
     ]
    }
   ],
   "source": [
    "# calculate the national error rate each year\n",
    "error_summary = (\n",
    "    df_all.groupby(\"year\")[\"error_flag\"]\n",
    "    .agg([\"count\", \"sum\"])\n",
    "    .rename(columns={\"count\": \"Total_Cases\", \"sum\": \"Error_Cases\"})\n",
    ")\n",
    "\n",
    "error_summary[\"Error_Rate\"] = error_summary[\"Error_Cases\"] / error_summary[\"Total_Cases\"]\n",
    "\n",
    "error_summary = error_summary.round(4)  # round to 4 decimal places\n",
    "print(error_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "09310123-a671-4593-b458-16e30d3ead40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      weighted_total  weighted_error  total_cases  error_cases  \\\n",
      "year                                                             \n",
      "2017    2.059688e+07    7.303715e+06      45530.0      16014.0   \n",
      "2018    1.972677e+07    7.091214e+06      43738.0      15506.0   \n",
      "2019    1.880200e+07    6.775693e+06      43258.0      15388.0   \n",
      "2020    1.935244e+07    6.793964e+06      27112.0       9322.0   \n",
      "2021    2.077275e+07    7.636574e+06       9832.0       3463.0   \n",
      "2022    2.071737e+07    8.060896e+06      41391.0      15611.0   \n",
      "2023    2.137528e+07    8.556200e+06      43776.0      16906.0   \n",
      "\n",
      "      weighted_error_rate  unweighted_error_rate  \n",
      "year                                              \n",
      "2017               0.3546                 0.3517  \n",
      "2018               0.3595                 0.3545  \n",
      "2019               0.3604                 0.3557  \n",
      "2020               0.3511                 0.3438  \n",
      "2021               0.3676                 0.3522  \n",
      "2022               0.3891                 0.3772  \n",
      "2023               0.4003                 0.3862  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\27311\\AppData\\Local\\Temp\\ipykernel_26632\\3095912405.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "# Calculate the weighted national error rate each year\n",
    "# Using FYWGT as case weight (full-year weight from QC codebook)\n",
    "\n",
    "error_summary_wt = (\n",
    "    df_all.groupby(\"year\")\n",
    "    .apply(lambda x: pd.Series({\n",
    "        \"weighted_total\": x[\"fywgt\"].sum(),\n",
    "        \"weighted_error\": (x[\"error_flag\"] * x[\"fywgt\"]).sum(),\n",
    "        \"total_cases\": len(x),\n",
    "        \"error_cases\": x[\"error_flag\"].sum()\n",
    "    }))\n",
    ")\n",
    "\n",
    "error_summary_wt[\"weighted_error_rate\"] = (\n",
    "    error_summary_wt[\"weighted_error\"] / error_summary_wt[\"weighted_total\"]\n",
    ")\n",
    "error_summary_wt[\"unweighted_error_rate\"] = (\n",
    "    error_summary_wt[\"error_cases\"] / error_summary_wt[\"total_cases\"]\n",
    ")\n",
    "\n",
    "error_summary_wt = error_summary_wt.round(4)\n",
    "\n",
    "print(error_summary_wt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec210c-f523-45ce-8b4b-351da4f66f5f",
   "metadata": {},
   "source": [
    "***Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ad0c6b90-f3d2-4d3c-a6e0-72ba2fdeba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found an unexpeted value from \"wrkred1\" variable, therefor I decide to replace it.\n",
    "df_all['wrkreg1'] = df_all['wrkreg1'].replace({6: 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2d5ec969-2518-4c89-8513-0efe6f630ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\27311\\AppData\\Local\\Temp\\ipykernel_26632\\2020590630.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[\"children_present\"] = (df_all[\"fsnkid\"] > 0).astype(int)\n",
      "C:\\Users\\27311\\AppData\\Local\\Temp\\ipykernel_26632\\2020590630.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[\"elderly_present\"] = (df_all[\"fsnelder\"] > 0).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# add the dummies for the present of children and erlder\n",
    "df_all[\"children_present\"] = (df_all[\"fsnkid\"] > 0).astype(int)\n",
    "df_all[\"elderly_present\"] = (df_all[\"fsnelder\"] > 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7106367c-7e8a-4da6-a87c-8b1303d27bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a map from FIPS code to state dummy \n",
    "state_map = {\n",
    "    1: \"Alabama\", 2: \"Alaska\", 4: \"Arizona\", 5: \"Arkansas\", 6: \"California\",\n",
    "    8: \"Colorado\", 9: \"Connecticut\", 10: \"Delaware\", 11: \"District_of_Columbia\",\n",
    "    12: \"Florida\", 13: \"Georgia\", 15: \"Hawaii\", 16: \"Idaho\", 17: \"Illinois\",\n",
    "    18: \"Indiana\", 19: \"Iowa\", 20: \"Kansas\", 21: \"Kentucky\", 22: \"Louisiana\",\n",
    "    23: \"Maine\", 24: \"Maryland\", 25: \"Massachusetts\", 26: \"Michigan\",\n",
    "    27: \"Minnesota\", 28: \"Mississippi\", 29: \"Missouri\", 30: \"Montana\",\n",
    "    31: \"Nebraska\", 32: \"Nevada\", 33: \"New_Hampshire\", 34: \"New_Jersey\",\n",
    "    35: \"New_Mexico\", 36: \"New_York\", 37: \"North_Carolina\", 38: \"North_Dakota\",\n",
    "    39: \"Ohio\", 40: \"Oklahoma\", 41: \"Oregon\", 42: \"Pennsylvania\",\n",
    "    44: \"Rhode_Island\", 45: \"South_Carolina\", 46: \"South_Dakota\",\n",
    "    47: \"Tennessee\", 48: \"Texas\", 49: \"Utah\", 50: \"Vermont\", 51: \"Virginia\",\n",
    "    53: \"Washington\", 54: \"West_Virginia\", 55: \"Wisconsin\", 56: \"Wyoming\",\n",
    "    66: \"Guam\", 78: \"Virgin_Islands\"\n",
    "}\n",
    "\n",
    "# build a map for other variables\n",
    "dummy_rename = {\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # COMPOSITION (unit composition)\n",
    "    # ---------------------------------------------\n",
    "    # 'composition_1.0': 'Comp_ChildrenOnly',\n",
    "    # 'composition_2.0': 'Comp_Children_OneMaleAdult',\n",
    "    # 'composition_3.0': 'Comp_Children_OneFemaleAdult',\n",
    "    # 'composition_4.0': 'Comp_Children_MarriedAdults',\n",
    "    # 'composition_5.0': 'Comp_Children_MultipleAdults',\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # SEX1 (sex of household head)\n",
    "    # 1=Male, 2=Female, 3=Unknown/Other\n",
    "    # ---------------------------------------------\n",
    "    'sex1_2.0': 'Female',\n",
    "    'sex1_3.0': 'Sex_Unknown',\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # EMPSTAL (employment status)\n",
    "    # 1=Not in labor force\n",
    "    # 2=Unemployed, seeking work\n",
    "    # 3=Active-duty military\n",
    "    # 4=Migrant farm labor\n",
    "    # 5=Non-migrant farm labor\n",
    "    # 6=Self-employed farming\n",
    "    # 7=Self-employed nonfarming\n",
    "    # 8=Employed by others\n",
    "    # ---------------------------------------------\n",
    "    'empsta1_2.0': 'Emp_Unemployed',\n",
    "    'empsta1_3.0': 'Emp_Military',\n",
    "    'empsta1_4.0': 'Emp_MigrantFarm',\n",
    "    'empsta1_5.0': 'Emp_NonMigrantFarm',\n",
    "    'empsta1_6.0': 'Emp_SelfEmp_Farm',\n",
    "    'empsta1_7.0': 'Emp_SelfEmp_NonFarm',\n",
    "    'empsta1_8.0': 'Emp_EmployedByOthers',\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # WRKREG1 (work registration)\n",
    "    # 1=work registrant\n",
    "    # 2=Exempt, physically or mentally unfit for employment\n",
    "    # 3=Exempt, care of a child under six or an incapacitated person\n",
    "    # 4=Exempt, working and/or earning the equivalent of 30 hours per week\n",
    "    # 5=Exempt, other\n",
    "    # ---------------------------------------------\n",
    "    'wrkreg1_2.0': 'WorkReg_Exempt_Participating',\n",
    "    'wrkreg1_3.0': 'WorkReg_NotExempt_NotPartic',\n",
    "    'wrkreg1_4.0': 'WorkReg_NotExempt_Participating',\n",
    "    'wrkreg1_5.0': 'WorkReg_Unknown',\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # REP_SYS (reporting requirement)\n",
    "    # 1 = $25 reporting\n",
    "    # 2 = $80 reporting\n",
    "    # 3 = $100 reporting\n",
    "    # 4 = Status reporting\n",
    "    # 5 = 5+ hour change reporting\n",
    "    # 6 = Simplified reporting\n",
    "    # 7 = Quarterly reporting\n",
    "    # 8 = Monthly reporting\n",
    "    # 9 = Transitional/no reporting\n",
    "    # 10 = Other\n",
    "    # ---------------------------------------------\n",
    "    # 'rep_sys_2.0': 'RepSys_80Dollar',\n",
    "    # 'rep_sys_3.0': 'RepSys_100Dollar',\n",
    "    # 'rep_sys_4.0': 'RepSys_Status',\n",
    "    # 'rep_sys_5.0': 'RepSys_HoursChange',\n",
    "    # 'rep_sys_6.0': 'RepSys_Simplified',\n",
    "    # 'rep_sys_7.0': 'RepSys_Quarterly',\n",
    "    # 'rep_sys_8.0': 'RepSys_Monthly',\n",
    "    # 'rep_sys_9.0': 'RepSys_Transitional',\n",
    "    # 'rep_sys_10.0': 'RepSys_Other',\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    #EXPEDSER- R - received expedited service\n",
    "    # 1 = Entitled to expedited service and received benefits within Federal time frame\n",
    "    # 2 = Entitled to expedited service but did not receive benefits within Federal time frame\n",
    "    # 3 = Not entitled to expedited service\n",
    "    # ---------------------------------------------\n",
    "    # 'expdeser_1.0': 'ExpeditedService_Yes',\n",
    "    'expedser_2.0': 'ExpeditedService_FederalTime',\n",
    "    'expedser_3.0': 'ExpeditedService_LateFederalTime',\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # CAT_ELIG (categorical eligibility)\n",
    "    # 0 = Not categorically eligible\n",
    "    # 1 = Eligible via PA/TANF etc\n",
    "    # 2 = Recoded eligible after review\n",
    "    # ---------------------------------------------\n",
    "    'cat_elig_1': 'CatEligible_PAorTANF',\n",
    "    'cat_elig_2': 'CatEligible_Recoded',\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # YRSED1 (education)\n",
    "    # 1 None\n",
    "    # 2 Grades 1–8\n",
    "    # 3 Grades 9–12, no diploma\n",
    "    # 4 GED/High school diploma\n",
    "    # 5 Some college\n",
    "    # 6 Associate degree\n",
    "    # 7 Bachelor’s\n",
    "    # 8 Graduate/professional degree\n",
    "    # But I put yrsed1 as a continuous variable\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "variable_rename = {\n",
    "\n",
    "    # Continuous\n",
    "    'ctprhh': 'Num_of_People_in_Household',\n",
    "    'fstotde2': 'ME_of_Total_Deduction',\n",
    "    'fsusize': 'HH_Size',\n",
    "    'children_present': 'children_present',\n",
    "    'elderly_present': 'elderly_present',\n",
    "    'fsndis': 'Num_Disabled',\n",
    "    'fsnetinc': 'Final_Net_Income',\n",
    "    'tpov': 'Income_to_Poverty_Ratio',\n",
    "    'certmth': 'Months_Since_Cert',\n",
    "\n",
    "    # Binary Vars\n",
    "    'fsasset': 'Asset_Test_Applicability',\n",
    "    'fsvehast': 'Vehicle_Test_Applicability',\n",
    "    'abwdst1': 'Able-Bodied_Adult_Without_Dependents',\n",
    "    'ssi_cap': 'SSI_CAP_Participation',\n",
    "    'fsafil1': 'CaseAffiliation',\n",
    "    'authrep': 'Authorized_Representative',\n",
    "    'actntype': 'Cert_vs_Recert',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cbcc6bb9-f981-4cca-9dc8-e24a30f33f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 23, 25, 33, 36, 44, 50, 10, 11, 24, 34, 42, 51, 54, 78,  1, 12,\n",
       "       13, 21, 28, 37, 45, 47, 17, 18, 26, 27, 39, 55,  5, 22, 35, 40, 48,\n",
       "        8, 19, 20, 29, 30, 31, 38, 46, 49, 56,  2,  4,  6, 15, 16, 32, 41,\n",
       "       53, 66], dtype=int8)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d4d34a13-de0c-4df7-a5cf-e0ec27c78266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the state from numbers to ab. names\n",
    "df_all['state'] = df_all['state'].map(state_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a01f8e09-d8ba-40d0-b2ee-5e1a71e9db24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\27311\\AppData\\Local\\Temp\\ipykernel_26632\\3591726586.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all['case_id'] = range(1, len(df_all) + 1)\n"
     ]
    }
   ],
   "source": [
    "# case id\n",
    "df_all['case_id'] = range(1, len(df_all) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e249c-43ab-4747-998e-3971f07b45e3",
   "metadata": {},
   "source": [
    "==========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "57f06788-2959-4779-8122-6d85d917dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the AGENCY(1-99) values\n",
    "agency_map = {\n",
    "    1: \"information not reported\",\n",
    "    2: \"incomplete or incorrect information provided; agency not required to verify\",\n",
    "    3: \"information withheld by client (case referred for intentional program violation investigation)\",\n",
    "    4: \"incorrect information provided by client (case referred for ipv investigation)\",\n",
    "    7: \"inaccurate information reported by collateral contact\",\n",
    "    8: \"acted on incorrect federal computer match information not requiring verification\",\n",
    "    10: \"policy incorrectly applied\",\n",
    "    12: \"reported information disregarded or not applied\",\n",
    "    14: \"agency failed to follow up on inconsistent or incomplete information\",\n",
    "    15: \"agency failed to follow up on impending changes\",\n",
    "    16: \"agency failed to verify required information\",\n",
    "    17: \"computer programming error\",\n",
    "    18: \"data entry and/or coding error\",\n",
    "    19: \"mass change (error due to problem with computer-generated mass change)\",\n",
    "    20: \"arithmetic computation error\",\n",
    "    21: \"computer user error\",\n",
    "    99: \"other\"\n",
    "}\n",
    "\n",
    "for i in range(1, 10):\n",
    "    col = f\"agency{i}\"\n",
    "    label_col = f\"agency{i}\"\n",
    "    df_all[label_col] = df_all[col].map(agency_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2b311dde-1bbf-4144-89de-99811be6e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the e_findg1-9\n",
    "e_findg_map = {\n",
    "    2: \"overissuance\",\n",
    "    3: \"underissuance\",\n",
    "    4: \"ineligible\"\n",
    "}\n",
    "\n",
    "for i in range(1, 10):\n",
    "    col = f\"e_findg{i}\"\n",
    "    label_col = f\"e_findg{i}\"\n",
    "    df_all[label_col] = df_all[col].map(e_findg_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "79bcdf29-6b03-4bec-b454-fbb0ab624acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the element1-9\n",
    "element_map = {\n",
    "    111: \"student status\",\n",
    "    130: \"citizenship and noncitizen status\",\n",
    "    140: \"residency\",\n",
    "    150: \"unit composition\",\n",
    "    151: \"recipient disqualification\",\n",
    "    160: \"employment and training programs\",\n",
    "    161: \"time-limited participation\",\n",
    "    162: \"work registration requirements\",\n",
    "    163: \"voluntary quit/reduced work effort\",\n",
    "    164: \"workfare and comparable workfare\",\n",
    "    165: \"employment status/job availability\",\n",
    "    166: \"acceptance of employment\",\n",
    "    170: \"social security number\",\n",
    "    211: \"bank accounts or cash on hand\",\n",
    "    212: \"nonrecurring lump-sum payment\",\n",
    "    213: \"other liquid assets\",\n",
    "    221: \"real property\",\n",
    "    222: \"vehicles\",\n",
    "    224: \"other nonliquid resources\",\n",
    "    225: \"combined resources\",\n",
    "    311: \"wages and salaries\",\n",
    "    312: \"self-employment\",\n",
    "    314: \"other earned income\",\n",
    "    321: \"earned income deductions\",\n",
    "    323: \"dependent care deduction\",\n",
    "    331: \"rsdi benefits\",\n",
    "    332: \"veterans' benefits\",\n",
    "    333: \"ssi and/or state ssi supplement\",\n",
    "    334: \"unemployment compensation\",\n",
    "    335: \"workers' compensation\",\n",
    "    336: \"other government benefits\",\n",
    "    342: \"contributions\",\n",
    "    343: \"deemed income\",\n",
    "    344: \"tanf, pa, or ga\",\n",
    "    345: \"educational grants/scholarships/loans\",\n",
    "    346: \"other unearned income\",\n",
    "    350: \"child support payments received from absent parent\",\n",
    "    361: \"standard deduction\",\n",
    "    363: \"shelter deduction\",\n",
    "    364: \"standard utility allowance\",\n",
    "    365: \"medical expense deductions\",\n",
    "    366: \"child support payment deduction\",\n",
    "    371: \"combined gross income\",\n",
    "    372: \"combined net income\",\n",
    "    520: \"arithmetic computation\",\n",
    "    530: \"transitional benefits\",\n",
    "    560: \"reporting systems\",\n",
    "    810: \"snap simplification project\",\n",
    "    820: \"demonstration projects\"\n",
    "}\n",
    "\n",
    "for i in range(1, 10):\n",
    "    col = f\"element{i}\"\n",
    "    df_all[col] = df_all[col].map(element_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3d37c2cf-2889-430d-990e-dd19191684b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the nature\n",
    "nature_map = {\n",
    "    6: \"eligible person(s) excluded\",\n",
    "    7: \"ineligible person(s) included\",\n",
    "    12: \"eligible person(s) with no income, resources, or deductible expenses excluded\",\n",
    "    13: \"eligible person(s) with income excluded\",\n",
    "    14: \"eligible person(s) with resources excluded\",\n",
    "    15: \"eligible person(s) with deductible expenses excluded\",\n",
    "    16: \"newborn improperly excluded\",\n",
    "    20: \"incorrect resource limit applied\",\n",
    "    24: \"resource should have been excluded\",\n",
    "    28: \"incorrect income limit applied\",\n",
    "    29: \"exceeds prescribed limit\",\n",
    "    30: \"resource should have been included\",\n",
    "    32: \"failed to consider or incorrectly considered income of ineligible member\",\n",
    "    35: \"unreported source of income\",\n",
    "    36: \"rounding used/not used or incorrectly applied\",\n",
    "    37: \"all income from source known but not included\",\n",
    "    38: \"more income received from this source than budgeted\",\n",
    "    39: \"employment status changed from unemployed to employed\",\n",
    "    40: \"employment status changed from employed to unemployed\",\n",
    "    41: \"change only in amount of earnings\",\n",
    "    42: \"conversion to monthly amount not used or incorrectly applied\",\n",
    "    43: \"averaging not used or incorrectly applied\",\n",
    "    44: \"less income received from this source than budgeted\",\n",
    "    45: \"cost of doing business not used or incorrectly applied\",\n",
    "    46: \"failed to consider/anticipate month with extra pay date\",\n",
    "    52: \"deduction that should have been included was not\",\n",
    "    53: \"deduction included that should not have been\",\n",
    "    54: \"incorrect standard used (not as a result of change in unit size or move)\",\n",
    "    64: \"incorrect amount used resulting from change in residence\",\n",
    "    65: \"incorrect standard used resulting from change in unit size\",\n",
    "    75: \"benefit/allotment/eligibility incorrectly computed\",\n",
    "    77: \"unit not entitled to transitional benefits\",\n",
    "    79: \"incorrect use of allotment tables\",\n",
    "    80: \"improper prorating of initial month's benefits\",\n",
    "    97: \"not required to be reported or acted upon based on time frames and reporting requirements for allotment differences below the error threshold\",\n",
    "    98: \"transcription or computation errors\",\n",
    "    99: \"other\",\n",
    "    111: \"child support payment(s) not considered or incorrectly applied\",\n",
    "    112: \"retained child support payment(s) not considered or incorrectly applied\",\n",
    "    120: \"variances/errors resulting from noncompliance with this means-tested public assistance program\",\n",
    "    123: \"incorrectly prorated\",\n",
    "    124: \"variances resulting from use of automatic federal information exchange system\",\n",
    "    127: \"pass-through not considered or incorrectly applied\",\n",
    "    200: \"eligible noncitizen excluded\",\n",
    "    201: \"ineligible noncitizen included\",\n",
    "    301: \"unit improperly participating under retrospective budgeting\",\n",
    "    302: \"unit improperly participating under prospective budgeting\",\n",
    "    303: \"unit improperly participating under monthly reporting\",\n",
    "    304: \"unit improperly participating under quarterly reporting\",\n",
    "    305: \"unit improperly participating under semiannual reporting\",\n",
    "    306: \"unit improperly participating under change reporting\",\n",
    "    307: \"unit improperly participating under status reporting\",\n",
    "    308: \"unit improperly participating under 5 hour reporting\",\n",
    "    309: \"unit improperly participating in transitional benefits\"\n",
    "}\n",
    "\n",
    "for i in range(1, 10):\n",
    "    col = f\"nature{i}\"\n",
    "    df_all[col] = df_all[col].map(nature_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f7632229-4b87-4dff-accd-2aecb8a48b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the time period\n",
    "timeper_map = {\n",
    "    1: \"before most recent action\",\n",
    "    2: \"at time of most recent action by agency\",\n",
    "    3: \"after most recent action by agency\",\n",
    "    9: \"time of occurrence cannot be determined\"\n",
    "}\n",
    "\n",
    "for i in range(1, 10):\n",
    "    col = f\"timeper{i}\"\n",
    "    df_all[col] = df_all[col].map(timeper_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "76b5f495-b390-4458-a38f-e5514b39ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the verif\n",
    "verif_map = {\n",
    "    1: \"verified\",\n",
    "    2: \"not verified\",\n",
    "    3: \"verification required but not provided\",\n",
    "    4: \"verification provided but incorrect or insufficient\",\n",
    "    5: \"verification not required\",\n",
    "    9: \"not applicable or cannot be determined\"\n",
    "}\n",
    "\n",
    "for i in range(1, 10):\n",
    "    col = f\"verif{i}\"\n",
    "    df_all[col] = df_all[col].map(verif_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5ebdc9a1-2664-47b7-bef8-ef9b0f062ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the case-level data\n",
    "status_map = {\n",
    "    1: \"amount correct\",\n",
    "    2: \"overissuance\",\n",
    "    3: \"underissuance\"\n",
    "}\n",
    "\n",
    "alladj_map = {\n",
    "    1: \"no adjustment\",\n",
    "    2: \"prorated benefit\",\n",
    "    3: \"other adjustment\"\n",
    "}\n",
    "\n",
    "case_map = {\n",
    "    1: \"included in error rate calculation\",\n",
    "    2: \"excluded—processed by ssa worker\",\n",
    "    3: \"excluded—fns designation\"\n",
    "}\n",
    "\n",
    "actntype_map = {\n",
    "    1: \"certification\",\n",
    "    2: \"recertification\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "df_all[\"status\"] = df_all[\"status\"].map(status_map)\n",
    "df_all[\"alladj\"] = df_all[\"alladj\"].map(alladj_map)\n",
    "df_all[\"case\"] = df_all[\"case\"].map(case_map)\n",
    "df_all[\"actntype\"] = df_all[\"actntype\"].map(actntype_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f5512-4c89-4897-82b0-c2c492705323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597c38e-1f93-4734-bb87-3f5e1204afb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70e84fc7-b939-4480-bc32-5477d679bc89",
   "metadata": {},
   "source": [
    "==============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f00f20ed-db13-4a39-95d0-d36e6391213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Feature preprocessing and model training\n",
    "# ============================================\n",
    "\n",
    "# Define variable type groups\n",
    "cont_vars = [\n",
    "    'ctprhh',        # household composition ratio\n",
    "    'fstotde2',      # case complexity\n",
    "    'fsusize',       # household size\n",
    "    'children_present', 'elderly_present', 'fsndis',   # counts of kids / elderly / disabled\n",
    "    'fsnetinc', 'tpov',              # net income & income-to-poverty ratio\n",
    "    'certmth'\n",
    "]\n",
    "binary_vars = [\n",
    "    'fsasset', 'fsvehast', 'abwdst1', 'ssi_cap', 'fsafil1', 'authrep',  # existing flags\n",
    "    'actntype'                               # newly added\n",
    "]\n",
    "cat_vars = [\n",
    "    'sex1', 'empsta1', 'wrkreg1',  # demographic/employment categories\n",
    "    'expedser', 'cat_elig', 'state'  # policy/admin + state FE\n",
    "]\n",
    "\n",
    "# ---------- new variables from the QC dataset ----------\n",
    "\n",
    "# 1. continuous variables\n",
    "cont_vars += [\n",
    "    # error-level continuous vars\n",
    "    *[f\"amount{i}\" for i in range(1, 10)],\n",
    "    *[f\"occdate{i}\" for i in range(1, 10)],\n",
    "    # case-level continuous vars\n",
    "    \"amtadj\",\n",
    "    \"amterr\",\n",
    "    \"lastcert\"\n",
    "]\n",
    "\n",
    "# 2. binary variables (none from QC dataset)\n",
    "\n",
    "# 3. categorical variables\n",
    "cat_vars += [\n",
    "    # error-level categorical vars\n",
    "    *[f\"agency{i}\" for i in range(1, 10)],\n",
    "    *[f\"e_findg{i}\" for i in range(1, 10)],\n",
    "    *[f\"element{i}\" for i in range(1, 10)],\n",
    "    *[f\"nature{i}\" for i in range(1, 10)],\n",
    "    *[f\"timeper{i}\" for i in range(1, 10)],\n",
    "    *[f\"verif{i}\" for i in range(1, 10)],\n",
    "\n",
    "    # case-level categorical vars\n",
    "    \"status\",\n",
    "    \"alladj\",\n",
    "    \"case\",\n",
    "    \"actntype\"   # already in binary list, but now clearly defined as categorical\n",
    "]\n",
    "\n",
    "# unique\n",
    "cont_vars = list(dict.fromkeys(cont_vars))\n",
    "binary_vars = list(dict.fromkeys(binary_vars))\n",
    "cat_vars = list(dict.fromkeys(cat_vars))\n",
    "\n",
    "# Final variable collection\n",
    "feature_cols = cont_vars + binary_vars + cat_vars\n",
    "target_col = 'error_flag'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7229d46a-9fc5-4a3d-9d49-66b860d9d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we want for the final model_variables dataset\n",
    "model_variables = df_all.copy()\n",
    "\n",
    "# keep the columns we want\n",
    "final_cols = ['case_id'] + feature_cols + [\"fywgt\", \"state\", \"year\", target_col]\n",
    "model_variables = model_variables[final_cols]\n",
    "\n",
    "# model_variables = model_variables.fillna('NA')\n",
    "\n",
    "model_variables.to_csv(\"model_variables_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca51a325-f5d4-4ce0-81dd-c6fbfb968e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e220a3cf-87e9-4881-a912-ea0aab949c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all the variables we need\n",
    "df_export = pd.concat([X, y], axis=1)\n",
    "\n",
    "df_export.to_csv(\"model_variables.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09f91a89-080d-4d27-87a4-fff5e1b2d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c87a3ebe-d369-48c9-b2ff-012edc1efce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the continuous and binary variables\n",
    "X = df_all[feature_cols].rename(columns=variable_rename)\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "X = pd.get_dummies(X, columns=cat_vars, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aafc9363-277e-4897-8b35-cb209c1d2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the dummy variables\n",
    "X = X.rename(columns=dummy_rename)\n",
    "\n",
    "# Fill missing values (simplify and avoid dimension mismatch)\n",
    "X = X.fillna(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
